{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e848af97-53d0-4d97-902c-756ffaf9fd43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:42:48.828508Z",
     "iopub.status.busy": "2025-10-01T06:42:48.828120Z",
     "iopub.status.idle": "2025-10-01T06:42:48.846883Z",
     "shell.execute_reply": "2025-10-01T06:42:48.846213Z",
     "shell.execute_reply.started": "2025-10-01T06:42:48.828482Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a19210-5263-45cc-bb6e-c15b6b39ce99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:42:48.848529Z",
     "iopub.status.busy": "2025-10-01T06:42:48.848039Z",
     "iopub.status.idle": "2025-10-01T06:42:48.897104Z",
     "shell.execute_reply": "2025-10-01T06:42:48.896345Z",
     "shell.execute_reply.started": "2025-10-01T06:42:48.848510Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_datalist = json.load(open(\"data_volumes/train_datalist.json\"))[:939]\n",
    "valid_datalist = json.load(open(\"data_volumes/valid_datalist.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450d0a7f-4816-412f-8ae3-9020de9b6402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:42:48.898712Z",
     "iopub.status.busy": "2025-10-01T06:42:48.898327Z",
     "iopub.status.idle": "2025-10-01T06:42:50.917181Z",
     "shell.execute_reply": "2025-10-01T06:42:50.916214Z",
     "shell.execute_reply.started": "2025-10-01T06:42:48.898678Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:54: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94861c9e-98d2-4647-a5e9-53b5e5c8d0b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:42:50.918688Z",
     "iopub.status.busy": "2025-10-01T06:42:50.918217Z",
     "iopub.status.idle": "2025-10-01T06:42:50.948213Z",
     "shell.execute_reply": "2025-10-01T06:42:50.947422Z",
     "shell.execute_reply.started": "2025-10-01T06:42:50.918666Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in train_datalist:\n",
    "    i['embeddings'] = f\"/home/jupyter/datasphere/project/data_volumes/dataset/embeddings/{i['name'].replace('.nii.gz', '.pt')}\"\n",
    "    \n",
    "for i in valid_datalist:\n",
    "    i['embeddings'] = f\"/home/jupyter/datasphere/project/data_volumes/dataset/embeddings_val/{i['name'].replace('.nii.gz', '.pt')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a50d3e-1838-482e-92ee-269731a476a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a22639-8ccd-4575-9d55-4e8da62d281f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T04:06:16.637400Z",
     "iopub.status.busy": "2025-10-01T04:06:16.637016Z",
     "iopub.status.idle": "2025-10-01T04:06:26.785239Z",
     "shell.execute_reply": "2025-10-01T04:06:26.784397Z",
     "shell.execute_reply.started": "2025-10-01T04:06:16.637369Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "2025-10-01 04:06:22.797086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-01 04:06:23.625925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinUNETR(\n",
       "  (swinViT): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(2, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers1): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "              (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=384, out_features=96, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers2): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers3): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers4): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder1): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(2, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(2, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder2): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder3): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder4): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder10): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder5): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder4): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(384, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder3): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(192, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder2): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(96, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder1): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (out): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(48, 43, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from monai.networks.nets import SwinUNETR\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# создаём модель\n",
    "model = SwinUNETR(\n",
    "    img_size=(96, 96, 96),\n",
    "    in_channels=2,\n",
    "    out_channels=43,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True,\n",
    ").to(device)\n",
    "\n",
    "# загрузка\n",
    "state_dict = torch.load('best_model.pth', map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02174ded-8137-45b6-8d94-87e55e5237a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T04:06:26.786956Z",
     "iopub.status.busy": "2025-10-01T04:06:26.786246Z",
     "iopub.status.idle": "2025-10-01T04:06:27.761407Z",
     "shell.execute_reply": "2025-10-01T04:06:27.759694Z",
     "shell.execute_reply.started": "2025-10-01T04:06:26.786934Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21610/4100347563.py\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAddGlobalResized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[1;32m     40\u001b[0m     \u001b[0mСоздаёт\u001b[0m \u001b[0mуменьшенную\u001b[0m \u001b[0mкопию\u001b[0m \u001b[0mвсего\u001b[0m \u001b[0mскана\u001b[0m \u001b[0mи\u001b[0m \u001b[0mкладёт\u001b[0m \u001b[0mеё\u001b[0m \u001b[0mпод\u001b[0m \u001b[0mновый\u001b[0m \u001b[0mключ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Transform' is not defined"
     ]
    }
   ],
   "source": [
    "## import os\n",
    "import torch, os\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, Orientationd, \n",
    "    Spacingd, ScaleIntensityRanged, EnsureTyped\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "\n",
    "# ---------------------\n",
    "# Настройки\n",
    "# ---------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "save_dir = \"/home/jupyter/datasphere/project/data_volumes/dataset/embeddings\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "roi_size = (96, 96, 96)\n",
    "overlap = 0.25\n",
    "TARGET_SIZE = (512, 512, 512)\n",
    "\n",
    "# ---------------------\n",
    "# Модель (только swinViT часть нужна)\n",
    "# ---------------------\n",
    "model = SwinUNETR(\n",
    "    img_size=roi_size,\n",
    "    in_channels=2,\n",
    "    out_channels=43,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# ---------------------\n",
    "# Трансформы\n",
    "# ---------------------\n",
    "\n",
    "class AddGlobalResized(Transform):\n",
    "    \"\"\"\n",
    "    Создаёт уменьшенную копию всего скана и кладёт её под новый ключ.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_key=\"image\", target_key=\"global\", size=96):\n",
    "        self.source_key = source_key\n",
    "        self.target_key = target_key\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        img = d[self.source_key]\n",
    "        global_resized = F.interpolate(\n",
    "            img.unsqueeze(0), size=(self.size, self.size, self.size),\n",
    "            mode=\"trilinear\", align_corners=False\n",
    "        ).squeeze(0)  # [C, size, size, size]\n",
    "        d[self.target_key] = global_resized\n",
    "        return d\n",
    "\n",
    "\n",
    "class AppendGlobalChannel(Transform):\n",
    "    \"\"\"\n",
    "    Добавляет глобальный канал (d[\"global\"]) к каждому кропу d[\"image\"].\n",
    "    Работает и если RandCrop вернул список словарей.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_key=\"image\", global_key=\"global\"):\n",
    "        self.image_key = image_key\n",
    "        self.global_key = global_key\n",
    "\n",
    "    def _add_channel(self, d):\n",
    "        d = dict(d)\n",
    "        patch_img = d[self.image_key]\n",
    "        global_resized = d[self.global_key]\n",
    "        # конкат по каналам\n",
    "        d[self.image_key] = torch.cat([patch_img, global_resized], dim=0)\n",
    "        return d\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            return [self._add_channel(d) for d in data]\n",
    "        return self._add_channel(data)\n",
    "\n",
    "transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\"], ensure_channel_first=True),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "    Spacingd(keys=[\"image\"], pixdim=(1.0,1.0,1.0), mode=\"bilinear\"),\n",
    "    ScaleIntensityRanged(keys=[\"image\"], a_min=-1000, a_max=400, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ResizeWithPadOrCropd(keys=[\"image\"], spatial_size=TARGET_SIZE),\n",
    "    AddGlobalResized(),\n",
    "    EnsureTyped(keys=[\"image\"])\n",
    "])\n",
    "\n",
    "# ---------------------\n",
    "# Датасет\n",
    "# ---------------------\n",
    "train_ds = Dataset(data=train_datalist, transform=transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# ---------------------\n",
    "# Кастомный sliding window\n",
    "# ---------------------\n",
    "def custom_sliding_embeddings_encoders(model, volume, global_, roi_size=(96,96,96), overlap=0.25, device=\"cuda\"):\n",
    "    _, _, D, H, W = volume.shape\n",
    "    d, h, w = roi_size\n",
    "\n",
    "    stride_d = int(d * (1 - overlap))\n",
    "    stride_h = int(h * (1 - overlap))\n",
    "    stride_w = int(w * (1 - overlap))\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for z in range(0, D, stride_d):\n",
    "            for y in range(0, H, stride_h):\n",
    "                for x in range(0, W, stride_w):\n",
    "                    zz, yy, xx = min(z, D - d), min(y, H - h), min(x, W - w)\n",
    "                    patch = volume[:, :, zz:zz+d, yy:yy+h, xx:xx+w]\n",
    "                    patch = torch.cat([patch, global_], dim=1).to(device)\n",
    "\n",
    "                    # 1) прогон через swinViT\n",
    "                    hidden_states = model.swinViT(patch)\n",
    "\n",
    "                    # 2) энкодеры\n",
    "                    enc0 = model.encoder1(patch)             # low-level\n",
    "                    enc1 = model.encoder2(hidden_states[0])\n",
    "                    enc2 = model.encoder3(hidden_states[1])\n",
    "                    enc3 = model.encoder4(hidden_states[2])\n",
    "                    enc4 = model.encoder10(hidden_states[4])\n",
    "\n",
    "                    # 3) spatial pooling\n",
    "                    pooled = [\n",
    "                        enc0.mean(dim=[2,3,4]),\n",
    "                        enc1.mean(dim=[2,3,4]),\n",
    "                        enc2.mean(dim=[2,3,4]),\n",
    "                        enc3.mean(dim=[2,3,4]),\n",
    "                        enc4.mean(dim=[2,3,4]),\n",
    "                    ]\n",
    "\n",
    "                    concat = torch.cat(pooled, dim=-1)   # [B, feat_dim]\n",
    "                    embeddings.append(concat.cpu())\n",
    "\n",
    "                    del patch, hidden_states, enc0, enc1, enc2, enc3, enc4, pooled, concat\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "    embeddings = torch.cat(embeddings, dim=0)  # [num_patches, feat_dim]\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# Прогон по датасету\n",
    "# ---------------------\n",
    "for i, batch in enumerate(train_loader):\n",
    "    img = batch[\"image\"].to(device) \n",
    "    global_ = batch[\"global\"].to(device) # [1,1,D,H,W]\n",
    "    emb = custom_sliding_embeddings_encoders(model, img, global_, roi_size=roi_size, overlap=overlap, device=device)\n",
    "\n",
    "    fname = os.path.basename(batch[\"name\"][0])\n",
    "    save_path = os.path.join(save_dir, fname.replace('.nii.gz', \".pt\"))\n",
    "\n",
    "    torch.save(emb, save_path)\n",
    "    print(f\"[{i+1}/{len(train_loader)}] Saved embedding {emb.shape} → {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32bf3d-97f7-459d-a737-48b31884f3c7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-01T04:06:27.762139Z",
     "iopub.status.idle": "2025-10-01T04:06:27.762440Z",
     "shell.execute_reply": "2025-10-01T04:06:27.762322Z",
     "shell.execute_reply.started": "2025-10-01T04:06:27.762306Z"
    }
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3891308-1fc9-4f06-b38e-e5cc516b0436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad70259-48d6-47f2-892c-77fee59c8b69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:42:50.949812Z",
     "iopub.status.busy": "2025-10-01T06:42:50.949455Z",
     "iopub.status.idle": "2025-10-01T06:42:53.528689Z",
     "shell.execute_reply": "2025-10-01T06:42:53.524681Z",
     "shell.execute_reply.started": "2025-10-01T06:42:50.949792Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# токенайзер и модель\n",
    "tokenizer = BartTokenizer.from_pretrained(\"Mahalingam/DistilBart-Med-Summary\")\n",
    "bart = BartForConditionalGeneration.from_pretrained(\"Mahalingam/DistilBart-Med-Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f1fb0e-b117-4795-97b0-300c0dce5800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:42:53.530450Z",
     "iopub.status.busy": "2025-10-01T06:42:53.530043Z",
     "iopub.status.idle": "2025-10-01T06:43:01.037086Z",
     "shell.execute_reply": "2025-10-01T06:43:01.035963Z",
     "shell.execute_reply.started": "2025-10-01T06:42:53.530428Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 06:42:58.896066: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-01 06:42:59.720565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, Spacingd, Orientationd, ScaleIntensityRanged,\n",
    "    CropForegroundd, RandFlipd, RandRotate90d, RandGaussianNoised,\n",
    "    RandAdjustContrastd, RandShiftIntensityd, RandCoarseDropoutd,\n",
    "    EnsureTyped, ToTensord, RandAffined, ResizeWithPadOrCropd, RandCropByPosNegLabeld\n",
    ")\n",
    "from monai.transforms import Transform\n",
    "\n",
    "# --- кастом: загрузка сохранённых эмбеддингов ---\n",
    "class LoadEmbeddingD(Transform):\n",
    "    def __call__(self, data):\n",
    "        if \"embeddings\" in data:\n",
    "            path = data[\"embeddings\"]\n",
    "            if path.endswith(\".pt\") or path.endswith(\".pth\"):\n",
    "                emb = torch.load(path, map_location=\"cpu\")\n",
    "            elif path.endswith(\".npy\"):\n",
    "                emb = torch.from_numpy(np.load(path))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported embedding format: {path}\")\n",
    "            data[\"embeddings\"] = emb.float()\n",
    "        return data\n",
    "\n",
    "# --- кастом трансформ для labels ---\n",
    "class CastLabelsToFloatD(Transform):\n",
    "    def __call__(self, data):\n",
    "        if \"labels\" in data:\n",
    "            data[\"labels\"] = np.asarray(data[\"labels\"]).astype(np.float32)\n",
    "        return data\n",
    "# --- кастом трансформ для текста ---\n",
    "class TokenizeReportD(Transform):\n",
    "    def __init__(self, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, data):\n",
    "        text = data.get(\"report\", \"\")\n",
    "\n",
    "        # report может быть dict (finding+impression) или str\n",
    "        if isinstance(text, dict):\n",
    "            combined = text.get(\"finding\", \"\") + \" \" + text.get(\"impression\", \"\")\n",
    "        else:\n",
    "            combined = str(text)\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            combined,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # для BART labels нужны (копия input_ids, паддинги заменяем на -100, чтобы не учитывались в лоссе)\n",
    "        labels = tokens[\"input_ids\"].clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        data[\"input_ids\"] = tokens[\"input_ids\"].squeeze(0)         # [seq_len]\n",
    "        data[\"attention_mask\"] = tokens[\"attention_mask\"].squeeze(0)  # [seq_len]\n",
    "        data[\"bart_labels\"] = labels.squeeze(0)                         # [seq_len]\n",
    "        return data\n",
    "\n",
    "\n",
    "# ----------------- Параметры -------------------\n",
    "TARGET_SPACING = (1.0, 1.0, 1.0)   # mm\n",
    "TARGET_SIZE = (512, 512, 512)      # для валида и инференса\n",
    "PATCH_SIZE = (96, 96, 96)       # патчи для тренировки\n",
    "HU_MIN, HU_MAX = -1000.0, 400.0    # окно легких\n",
    "\n",
    "\n",
    "\n",
    "# --- трансформы для multimodal (целый скан для эмбеддингов) ---\n",
    "multimodal_transforms = Compose([\n",
    "    TokenizeReportD(tokenizer=tokenizer, max_length=512),\n",
    "    CastLabelsToFloatD(),\n",
    "    LoadImaged(keys=[\"image\", \"mask\"], ensure_channel_first=True),\n",
    "    Orientationd(keys=[\"image\", \"mask\"], axcodes=\"RAS\"),\n",
    "    Spacingd(keys=[\"image\", \"mask\"], pixdim=TARGET_SPACING, mode=(\"bilinear\", \"nearest\")),\n",
    "    CropForegroundd(keys=[\"image\", \"mask\"], source_key=\"image\"),\n",
    "    ScaleIntensityRanged(keys=[\"image\"], a_min=HU_MIN, a_max=HU_MAX, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ResizeWithPadOrCropd(keys=[\"image\", \"mask\"], spatial_size=TARGET_SIZE),\n",
    "    EnsureTyped(keys=[\"image\", \"mask\"]),\n",
    "    ToTensord(keys=[\"image\", \"mask\", \"labels\"]),\n",
    "])\n",
    "\n",
    "multimodal_embed_transforms = Compose([\n",
    "    LoadEmbeddingD(),                      # грузим эмбеддинги из файла\n",
    "    CastLabelsToFloatD(),                  # метки в float32\n",
    "    TokenizeReportD(tokenizer=tokenizer, max_length=512),  # токенизация текста\n",
    "    EnsureTyped(keys=[\"embeddings\", \"labels\"]),\n",
    "    ToTensord(keys=[\"embeddings\", \"labels\", \"input_ids\", \"attention_mask\", \"bart_labels\"]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d0c43d7-6251-4719-9347-8b880bee4743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:43:01.038982Z",
     "iopub.status.busy": "2025-10-01T06:43:01.038223Z",
     "iopub.status.idle": "2025-10-01T06:43:01.104299Z",
     "shell.execute_reply": "2025-10-01T06:43:01.103559Z",
     "shell.execute_reply.started": "2025-10-01T06:43:01.038959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: список элементов от __getitem__ датасета\n",
    "           каждый элемент — dict с:\n",
    "             - embeddings: [E]\n",
    "             - labels: [num_classes]\n",
    "             - input_ids: [seq_len]\n",
    "             - attention_mask: [seq_len]\n",
    "             - bart_labels: [seq_len]\n",
    "    \"\"\"\n",
    "\n",
    "    # собираем эмбеддинги и метки\n",
    "    embeddings = [sample[\"embeddings\"] for sample in batch]\n",
    "    # паддинг до максимальной длины в этом батче\n",
    "    embeddings = pad_sequence(embeddings, batch_first=True)  # [B, max_len, 1152]\n",
    "    labels = torch.stack([sample[\"labels\"] for sample in batch]).float()   # [B, num_classes]\n",
    "\n",
    "    # паддинг токенов по max длине\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [sample[\"input_ids\"] for sample in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "        [sample[\"attention_mask\"] for sample in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "    bart_labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        [sample[\"bart_labels\"] for sample in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=-100  # у BART паддинги должны быть -100\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"embeddings\": embeddings,        # [B, E]\n",
    "        \"labels\": labels,                # [B, num_classes]\n",
    "        \"input_ids\": input_ids,          # [B, L]\n",
    "        \"attention_mask\": attention_mask,# [B, L]\n",
    "        \"bart_labels\": bart_labels       # [B, L]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdbf88-78f8-4241-a78a-d225bc007819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e1140f1-c002-404b-b115-f5e77498c57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:43:01.105935Z",
     "iopub.status.busy": "2025-10-01T06:43:01.105596Z",
     "iopub.status.idle": "2025-10-01T06:43:15.503729Z",
     "shell.execute_reply": "2025-10-01T06:43:15.503050Z",
     "shell.execute_reply.started": "2025-10-01T06:43:01.105915Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 939/939 [00:13<00:00, 68.99it/s]\n",
      "Loading dataset: 100%|██████████| 50/50 [00:00<00:00, 67.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from monai.data import Dataset\n",
    "from monai.data.dataset import CacheDataset\n",
    "\n",
    "train_ds = CacheDataset(data=train_datalist, transform=multimodal_embed_transforms, num_workers=8)\n",
    "val_ds = CacheDataset(data=valid_datalist, transform=multimodal_embed_transforms, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c0d6616-289a-4b4d-a3d0-9a2e5516ccf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T06:43:15.505231Z",
     "iopub.status.busy": "2025-10-01T06:43:15.504572Z",
     "iopub.status.idle": "2025-10-01T06:43:15.529184Z",
     "shell.execute_reply": "2025-10-01T06:43:15.528535Z",
     "shell.execute_reply.started": "2025-10-01T06:43:15.505209Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=50, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "557694b4-90fd-4213-aac7-a5a5ecb6ce1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T08:14:49.405312Z",
     "iopub.status.busy": "2025-10-01T08:14:49.404216Z",
     "iopub.status.idle": "2025-10-01T08:14:49.454463Z",
     "shell.execute_reply": "2025-10-01T08:14:49.453566Z",
     "shell.execute_reply.started": "2025-10-01T08:14:49.405280Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BartForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "\n",
    "# === Graph Attention Layer для CLS ===\n",
    "class SimpleGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout=0.1, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.a = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    def forward(self, h):\n",
    "        B, N, D = h.shape\n",
    "        Wh = self.W(h)  # [B, N, out_dim]\n",
    "        Wh_repeat1 = Wh.unsqueeze(2).repeat(1, 1, N, 1)  # [B, N, N, D]\n",
    "        Wh_repeat2 = Wh.unsqueeze(1).repeat(1, N, 1, 1)  # [B, N, N, D]\n",
    "        e = self.leakyrelu(self.a(torch.cat([Wh_repeat1, Wh_repeat2], dim=-1))).squeeze(-1)  # [B, N, N]\n",
    "\n",
    "        attn = F.softmax(e, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        h_prime = torch.bmm(attn, Wh)  # [B, N, out_dim]\n",
    "        return h_prime\n",
    "\n",
    "\n",
    "# === Cross-Attention Bridge c CLS и GEN потоками ===\n",
    "class CrossAttentionBridge(nn.Module):\n",
    "    def __init__(self, embed_dim, bart_dim, num_heads=8, num_cls=18, num_gen=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(embed_dim, bart_dim)\n",
    "        self.proj_norm = nn.LayerNorm(bart_dim)\n",
    "\n",
    "        self.cls_queries = nn.Parameter(torch.randn(1, num_cls, bart_dim))\n",
    "        self.gen_queries = nn.Parameter(torch.randn(1, num_gen, bart_dim))\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=bart_dim, num_heads=num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(bart_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=bart_dim, nhead=8, batch_first=True, dropout=dropout\n",
    "        )\n",
    "        self.cls_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.gen_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        self.gat = SimpleGATLayer(bart_dim, bart_dim, dropout=dropout)\n",
    "\n",
    "    def forward(self, patch_embeddings):\n",
    "        B = patch_embeddings.size(0)\n",
    "        patches = self.proj_norm(self.proj(patch_embeddings))\n",
    "\n",
    "        # CLS поток\n",
    "        cls_queries = self.cls_queries.expand(B, -1, -1)\n",
    "        cls_attended, _ = self.cross_attn(query=cls_queries, key=patches, value=patches)\n",
    "        cls_attended = self.norm(cls_attended + cls_queries)\n",
    "        cls_attended = self.dropout(cls_attended)\n",
    "        cls_attended = self.cls_encoder(cls_attended)\n",
    "        cls_attended = self.gat(cls_attended)\n",
    "\n",
    "        # GEN поток\n",
    "        gen_queries = self.gen_queries.expand(B, -1, -1)\n",
    "        gen_attended, _ = self.cross_attn(query=gen_queries, key=patches, value=patches)\n",
    "        gen_attended = self.norm(gen_attended + gen_queries)\n",
    "        gen_attended = self.dropout(gen_attended)\n",
    "        gen_attended = self.gen_encoder(gen_attended)\n",
    "\n",
    "        return cls_attended, gen_attended\n",
    "\n",
    "\n",
    "# === MoE Classifier ===\n",
    "class MoEClassifier(nn.Module):\n",
    "    def __init__(self, bart_dim, num_classes=18, num_experts=4, hidden_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(bart_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, num_classes)\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(bart_dim, num_experts),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cls_reprs):\n",
    "        pooled = cls_reprs.mean(dim=1)\n",
    "        gate_weights = self.gate(pooled)\n",
    "\n",
    "        expert_outputs = []\n",
    "        for expert in self.experts:\n",
    "            out = expert(pooled)\n",
    "            expert_outputs.append(out.unsqueeze(2))\n",
    "\n",
    "        expert_outputs = torch.cat(expert_outputs, dim=2)  # [B, num_classes, num_experts]\n",
    "        gate_weights = gate_weights.unsqueeze(1)           # [B, 1, num_experts]\n",
    "\n",
    "        logits = torch.bmm(expert_outputs, gate_weights.transpose(1, 2)).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# === Uncertainty Weighting ===\n",
    "class UncertaintyWeighting(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_vars = nn.Parameter(torch.zeros(2))\n",
    "\n",
    "    def forward(self, cls_loss, bart_loss):\n",
    "        cls_loss_weighted = torch.exp(-self.log_vars[0]) * cls_loss + self.log_vars[0]\n",
    "        bart_loss_weighted = torch.exp(-self.log_vars[1]) * bart_loss + self.log_vars[1]\n",
    "        return cls_loss_weighted + bart_loss_weighted\n",
    "\n",
    "\n",
    "# === Coverage Loss ===\n",
    "def compute_coverage_loss(cross_attentions, eps=1e-8):\n",
    "    attns = [att.mean(dim=1) for att in cross_attentions]  # [batch, tgt_len, src_len]\n",
    "    attn = torch.stack(attns).mean(dim=0)                  # [batch, tgt_len, src_len]\n",
    "\n",
    "    coverage = torch.zeros_like(attn[:, 0, :])\n",
    "    cov_loss = 0.0\n",
    "    for t in range(attn.size(1)):\n",
    "        step_attn = attn[:, t, :]\n",
    "        cov_loss += torch.sum(torch.min(step_attn, coverage), dim=-1).mean()\n",
    "        coverage = coverage + step_attn\n",
    "\n",
    "    return cov_loss / (attn.size(1) + eps)\n",
    "\n",
    "\n",
    "# === Основная модель ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BartForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "\n",
    "class MultiModalMonsterUltimate(nn.Module):\n",
    "    def __init__(self, bart_ckpt=\"facebook/bart-base\",\n",
    "                 embed_dim=1152, num_classes=18, dropout=0.1,\n",
    "                 num_experts=4, num_gen_queries=8,\n",
    "                 lambda_bart=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bart = BartForConditionalGeneration.from_pretrained(bart_ckpt)\n",
    "\n",
    "        self.bridge = CrossAttentionBridge(\n",
    "            embed_dim=embed_dim,\n",
    "            bart_dim=self.bart.config.d_model,\n",
    "            num_heads=8,\n",
    "            num_cls=num_classes,\n",
    "            num_gen=num_gen_queries,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # оставляем MoE, но можно заменить на простой Linear для дебага\n",
    "        self.classifier = MoEClassifier(\n",
    "            bart_dim=self.bart.config.d_model,\n",
    "            num_classes=num_classes,\n",
    "            num_experts=num_experts,\n",
    "            hidden_dim=512,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_bart = lambda_bart  # вес генерации\n",
    "\n",
    "    def forward(self, embeddings, labels=None, bart_labels=None,\n",
    "                classification_loss=None, use_soft_prompt=False, tokenizer=None, topk=3):\n",
    "        out = {}\n",
    "\n",
    "        cls_repr, gen_repr = self.bridge(embeddings)\n",
    "\n",
    "        # --- Классификация ---\n",
    "        logits_cls = self.classifier(cls_repr)\n",
    "        out[\"logits_cls\"] = logits_cls\n",
    "\n",
    "        if labels is not None and classification_loss is not None:\n",
    "            cls_loss = classification_loss(logits_cls, labels.float())\n",
    "        else:\n",
    "            cls_loss = torch.tensor(0.0, device=embeddings.device)\n",
    "\n",
    "        # --- Генерация ---\n",
    "        bart_loss = torch.tensor(0.0, device=embeddings.device)\n",
    "        coverage_loss = torch.tensor(0.0, device=embeddings.device)\n",
    "        bridge_out = gen_repr\n",
    "\n",
    "        if bart_labels is not None:\n",
    "            if use_soft_prompt and tokenizer is not None:\n",
    "                probs = torch.sigmoid(logits_cls)\n",
    "                topk_idx = torch.topk(probs, k=min(topk, self.num_classes), dim=-1).indices[0].tolist()\n",
    "                prompt_text = \"Findings: \" + \", \".join([f\"pathology_{i}\" for i in topk_idx]) + \". Report:\"\n",
    "                prompt_tokens = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(embeddings.device)\n",
    "                bart_labels = torch.cat([prompt_tokens, bart_labels], dim=1)\n",
    "\n",
    "            bart_out = self.bart(\n",
    "                encoder_outputs=BaseModelOutput(last_hidden_state=bridge_out),\n",
    "                labels=bart_labels,\n",
    "                output_attentions=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "            bart_loss = bart_out.loss\n",
    "            coverage_loss = compute_coverage_loss(bart_out.cross_attentions)\n",
    "            out[\"bart_out\"] = bart_out\n",
    "\n",
    "        # --- Лоссы ---\n",
    "        out[\"cls_loss\"] = cls_loss\n",
    "        out[\"bart_loss\"] = bart_loss\n",
    "        out[\"coverage_loss\"] = coverage_loss\n",
    "\n",
    "        # фиксированный баланс\n",
    "        out[\"loss\"] = cls_loss + self.lambda_bart * (bart_loss + coverage_loss)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32acc090-9f0c-4734-9a73-2b8b303f4b8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T08:14:50.807544Z",
     "iopub.status.busy": "2025-10-01T08:14:50.807067Z",
     "iopub.status.idle": "2025-10-01T08:14:50.871928Z",
     "shell.execute_reply": "2025-10-01T08:14:50.871053Z",
     "shell.execute_reply.started": "2025-10-01T08:14:50.807516Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# BCEWithLogitsLoss с весами для мульти-лейбл классификации\n",
    "cls_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def step_fn(model, batch, optimizer=None, device=\"cuda\", val=False, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Один шаг тренировки или валидации для MultiModalMonsterUltimate.\n",
    "    Возвращает loss и метрики.\n",
    "    \"\"\"\n",
    "    model.train(optimizer is not None)\n",
    "    batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "    # прямой проход\n",
    "    outputs = model(\n",
    "        embeddings=batch['embeddings'],\n",
    "        labels=batch['labels'],\n",
    "        bart_labels=batch['bart_labels'],\n",
    "        classification_loss=cls_loss,\n",
    "        use_soft_prompt=True if val else False,  # во время валидации можно подсунуть prompt\n",
    "    )\n",
    "\n",
    "    loss = outputs[\"loss\"]\n",
    "\n",
    "    # шаг оптимизации\n",
    "    if optimizer:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    # метрики\n",
    "    metrics = {}\n",
    "    if val:\n",
    "        # === Классификация ===\n",
    "        if \"logits_cls\" in outputs and \"labels\" in batch:\n",
    "            preds = (torch.sigmoid(outputs[\"logits_cls\"]) > 0.5).int().cpu().numpy()\n",
    "            labels = batch[\"labels\"].cpu().numpy().astype(int)\n",
    "\n",
    "            metrics[\"f1_micro\"] = f1_score(labels, preds, average=\"micro\")\n",
    "            metrics[\"f1_macro\"] = f1_score(labels, preds, average=\"macro\")\n",
    "\n",
    "        # === Генерация текста ===\n",
    "        if \"bart_out\" in outputs and \"bart_labels\" in batch and tokenizer is not None:\n",
    "            # gold labels\n",
    "            refs = batch[\"bart_labels\"].detach().cpu().tolist()\n",
    "            refs = [[tok for tok in seq if tok is not None and tok != -100] for seq in refs]\n",
    "\n",
    "            # предсказания из логитов\n",
    "            preds = outputs[\"bart_out\"].logits.argmax(-1).detach().cpu().tolist()\n",
    "\n",
    "            # декодирование\n",
    "            refs_text = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "            preds_text = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "            bleu_scores = [\n",
    "                sentence_bleu([r.split()], h.split()) if len(r) > 0 and len(h) > 0 else 0.0\n",
    "                for r, h in zip(refs_text, preds_text)\n",
    "            ]\n",
    "            metrics[\"bleu\"] = sum(bleu_scores) / max(1, len(bleu_scores))\n",
    "\n",
    "        # === Coverage Loss ===\n",
    "        if \"coverage_loss\" in outputs:\n",
    "            metrics[\"coverage_loss\"] = outputs[\"coverage_loss\"].item()\n",
    "\n",
    "        # Отдельно лоссы\n",
    "        metrics[\"cls_loss\"] = outputs[\"cls_loss\"].item()\n",
    "        metrics[\"bart_loss\"] = outputs[\"bart_loss\"].item()\n",
    "\n",
    "    return loss.item(), metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9186794a-0cef-4c39-8b78-600161f0e99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T08:14:51.821289Z",
     "iopub.status.busy": "2025-10-01T08:14:51.820835Z",
     "iopub.status.idle": "2025-10-01T08:14:51.841031Z",
     "shell.execute_reply": "2025-10-01T08:14:51.840159Z",
     "shell.execute_reply.started": "2025-10-01T08:14:51.821261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"nltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acf9f3-1d04-4579-b490-3819884a551d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T08:14:52.509400Z",
     "iopub.status.busy": "2025-10-01T08:14:52.508875Z",
     "iopub.status.idle": "2025-10-01T09:45:51.281633Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [train]: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]\n",
      "Epoch 1 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 1 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000 || Train Loss: 2.3796 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.9772 | F1_micro: 0.2613 | F1_macro: 0.0380 | BLEU: 0.0000 || LRs: 9.999994e-05 | 9.999994e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch1_valBLEU0.0000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 3 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 4 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 5 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 6 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 7 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 8 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 9 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 10 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 11 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 11 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 11 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/2000 || Train Loss: 2.0285 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.8832 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || LRs: 9.999254e-05 | 9.999254e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 13 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 14 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 15 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 16 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 17 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 18 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 19 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 20 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 21 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 21 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 21 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/2000 || Train Loss: 1.9976 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.8578 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || LRs: 9.997280e-05 | 9.997280e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 23 [train]: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]\n",
      "Epoch 24 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 25 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 26 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 27 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 28 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.50s/it]\n",
      "Epoch 29 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 30 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 31 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 31 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 31 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/2000 || Train Loss: 1.9741 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.8374 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || LRs: 9.994073e-05 | 9.994073e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch31_valBLEU0.0000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 33 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 34 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 35 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 36 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 37 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 38 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 39 [train]: 100%|██████████| 15/15 [00:23<00:00,  1.55s/it]\n",
      "Epoch 40 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 41 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 41 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 41 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/2000 || Train Loss: 1.9424 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.7539 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || LRs: 9.989634e-05 | 9.989634e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 43 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 44 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 45 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 46 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 47 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 48 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 49 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 50 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 51 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 51 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 51 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/2000 || Train Loss: 1.5918 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.3124 | F1_micro: 0.1667 | F1_macro: 0.0292 | BLEU: 0.0271 || LRs: 9.983964e-05 | 9.983964e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch51_valBLEU0.0271.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 53 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 54 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 55 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 56 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 57 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 58 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 59 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 60 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 61 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 61 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 61 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/2000 || Train Loss: 1.3980 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.1659 | F1_micro: 0.1657 | F1_macro: 0.0287 | BLEU: 0.0573 || LRs: 9.977065e-05 | 9.977065e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch61_valBLEU0.0573.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 63 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 64 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 65 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 66 [train]: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]\n",
      "Epoch 67 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 68 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 69 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 70 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 71 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 71 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 71 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/2000 || Train Loss: 1.2876 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.0585 | F1_micro: 0.1724 | F1_macro: 0.0431 | BLEU: 0.0918 || LRs: 9.968937e-05 | 9.968937e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch71_valBLEU0.0918.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 73 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.50s/it]\n",
      "Epoch 74 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.50s/it]\n",
      "Epoch 75 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 76 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 77 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 78 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 79 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 80 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.50s/it]\n",
      "Epoch 81 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 81 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 81 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/2000 || Train Loss: 1.2147 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.0025 | F1_micro: 0.2069 | F1_macro: 0.0511 | BLEU: 0.1132 || LRs: 9.959583e-05 | 9.959583e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch81_valBLEU0.1132.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 83 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 84 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 85 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 86 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 87 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 88 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 89 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.50s/it]\n",
      "Epoch 90 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 91 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 91 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 91 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/2000 || Train Loss: 1.1462 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 0.9888 | F1_micro: 0.3143 | F1_macro: 0.1371 | BLEU: 0.1768 || LRs: 9.949006e-05 | 9.949006e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch91_valBLEU0.1768.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 93 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 94 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 95 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 96 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 97 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 98 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 99 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 100 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 101 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 101 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 101 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/2000 || Train Loss: 1.0899 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 0.9710 | F1_micro: 0.3544 | F1_macro: 0.2112 | BLEU: 0.1936 || LRs: 9.937207e-05 | 9.937207e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch101_valBLEU0.1936.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 103 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 104 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 105 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 106 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 107 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 108 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 109 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 110 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 111 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 111 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 111 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/2000 || Train Loss: 1.0099 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 0.9644 | F1_micro: 0.3320 | F1_macro: 0.1597 | BLEU: 0.2434 || LRs: 9.924190e-05 | 9.924190e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch111_valBLEU0.2434.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "Epoch 112 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 113 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 114 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 115 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 116 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 117 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 118 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 119 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 120 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 121 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 121 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/2000 || Train Loss: 0.9382 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 0.9890 | F1_micro: 0.4066 | F1_macro: 0.2967 | BLEU: 0.2570 || LRs: 9.909959e-05 | 9.909959e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch121_valBLEU0.2570.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 123 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 124 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 125 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 126 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 127 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 128 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 129 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 130 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 131 [train]: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]\n",
      "Epoch 131 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/2000 || Train Loss: 0.8400 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 0.9882 | F1_micro: 0.3789 | F1_macro: 0.3046 | BLEU: 0.2643 || LRs: 9.894515e-05 | 9.894515e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch131_valBLEU0.2643.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 133 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 134 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 135 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 136 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 137 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 138 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 139 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 140 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 141 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 141 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/2000 || Train Loss: 0.7573 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 0.9843 | F1_micro: 0.3569 | F1_macro: 0.2224 | BLEU: 0.2911 || LRs: 9.877864e-05 | 9.877864e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch141_valBLEU0.2911.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 143 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 144 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 145 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 146 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 147 [train]: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]\n",
      "Epoch 148 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 149 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 150 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 151 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 151 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/2000 || Train Loss: 0.6888 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.0467 | F1_micro: 0.3310 | F1_macro: 0.1941 | BLEU: 0.3033 || LRs: 9.860010e-05 | 9.860010e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 156 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 157 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 158 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 160 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 161 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 161 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/2000 || Train Loss: 0.6485 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.0401 | F1_micro: 0.3259 | F1_macro: 0.1782 | BLEU: 0.3084 || LRs: 9.840957e-05 | 9.840957e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch161_valBLEU0.3084.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 163 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 166 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 167 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 168 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 169 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 170 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 171 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 171 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/2000 || Train Loss: 0.6173 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.0444 | F1_micro: 0.3650 | F1_macro: 0.1733 | BLEU: 0.2809 || LRs: 9.820709e-05 | 9.820709e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 173 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 174 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 175 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 176 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 177 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 178 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 179 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 180 [train]: 100%|██████████| 15/15 [00:23<00:00,  1.55s/it]\n",
      "Epoch 181 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 181 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/2000 || Train Loss: 0.5943 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.1008 | F1_micro: 0.3213 | F1_macro: 0.1812 | BLEU: 0.3225 || LRs: 9.799271e-05 | 9.799271e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch181_valBLEU0.3225.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.51s/it]\n",
      "Epoch 183 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 184 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 185 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 186 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 187 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 188 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 189 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 190 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 191 [train]: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]\n",
      "Epoch 191 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/2000 || Train Loss: 0.5722 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.1278 | F1_micro: 0.3728 | F1_macro: 0.2163 | BLEU: 0.3250 || LRs: 9.776650e-05 | 9.776650e-07\n",
      "💾 Сохранён лучший чекпоинт: checkpoints/epoch191_valBLEU0.3250.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 193 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 194 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 195 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 201 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 201 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/2000 || Train Loss: 0.5590 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.2161 | F1_micro: 0.3427 | F1_macro: 0.2009 | BLEU: 0.3166 || LRs: 9.752850e-05 | 9.752850e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 202 [train]: 100%|██████████| 15/15 [00:23<00:00,  1.55s/it]\n",
      "Epoch 203 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 204 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 205 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 206 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 207 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 208 [train]: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]\n",
      "Epoch 209 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 210 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 211 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 211 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 211 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211/2000 || Train Loss: 0.5534 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.1495 | F1_micro: 0.3468 | F1_macro: 0.1860 | BLEU: 0.3238 || LRs: 9.727877e-05 | 9.727877e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 212 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 213 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 214 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 215 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 216 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 217 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 218 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 219 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 220 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 221 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 221 [val]:   0%|          | 0/1 [00:00<?, ?it/s]F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Epoch 221 [val]: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/2000 || Train Loss: 0.5473 | F1_micro: 0.0000 | F1_macro: 0.0000 | BLEU: 0.0000 || Val Loss: 1.1830 | F1_micro: 0.3309 | F1_macro: 0.1949 | BLEU: 0.2873 || LRs: 9.701738e-05 | 9.701738e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 222 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 223 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 224 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 225 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 226 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]\n",
      "Epoch 227 [train]: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]\n",
      "Epoch 228 [train]:  87%|████████▋ | 13/15 [00:21<00:03,  1.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35299/1791924920.py\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# --- Тренировка ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} [train]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtotal_f1_micro\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f1_micro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35299/1053582985.py\u001b[0m in \u001b[0;36mstep_fn\u001b[0;34m(model, batch, optimizer, device, val, tokenizer)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# метрики\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    185\u001b[0m             )\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_mul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_addcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# Delete the local intermediate since it won't be used anymore to save on peak memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Инициализация ===\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MultiModalMonsterUltimate().to(device)\n",
    "# model.load_state_dict(torch.load('best_monster.pth')[\"model_state\"])\n",
    "cls_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# разделяем параметры (BART vs остальные)\n",
    "bart_params, other_params = [], []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"bart\" in name:\n",
    "        bart_params.append(param)\n",
    "    else:\n",
    "        other_params.append(param)\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {\"params\": other_params, \"lr\": 1e-4, \"weight_decay\": 0.01},\n",
    "    {\"params\": bart_params, \"lr\": 1e-6, \"weight_decay\": 0.01}\n",
    "])\n",
    "\n",
    "num_epochs = 2000\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "save_dir = \"checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_val_bleu = -float(\"inf\")  # чем больше BLEU, тем лучше\n",
    "\n",
    "# === Цикл обучения ===\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_f1_micro, total_f1_macro, total_bleu = 0, 0, 0, 0\n",
    "    n_batches = len(train_loader)\n",
    "\n",
    "    # --- Тренировка ---\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} [train]\"):\n",
    "        loss, metrics = step_fn(model, batch, optimizer, device, val=False, tokenizer=tokenizer)\n",
    "        total_loss += loss\n",
    "        total_f1_micro += metrics.get(\"f1_micro\", 0)\n",
    "        total_f1_macro += metrics.get(\"f1_macro\", 0)\n",
    "        total_bleu += metrics.get(\"bleu\", 0)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / n_batches\n",
    "    avg_train_f1_micro = total_f1_micro / n_batches\n",
    "    avg_train_f1_macro = total_f1_macro / n_batches\n",
    "    avg_train_bleu = total_bleu / n_batches\n",
    "\n",
    "    # --- Валидация ---\n",
    "    avg_val_loss, avg_val_f1_micro, avg_val_f1_macro, avg_val_bleu = 0, 0, 0, 0\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [val]\"):\n",
    "                loss, metrics = step_fn(model, batch, None, device, val=True, tokenizer=tokenizer)\n",
    "                avg_val_loss += loss\n",
    "                avg_val_f1_micro += metrics.get(\"f1_micro\", 0)\n",
    "                avg_val_f1_macro += metrics.get(\"f1_macro\", 0)\n",
    "                avg_val_bleu += metrics.get(\"bleu\", 0)\n",
    "\n",
    "        avg_val_loss /= len(val_loader)\n",
    "        avg_val_f1_micro /= len(val_loader)\n",
    "        avg_val_f1_macro /= len(val_loader)\n",
    "        avg_val_bleu /= len(val_loader)\n",
    "\n",
    "        # --- лог ---\n",
    "        lrs = [pg[\"lr\"] for pg in optimizer.param_groups]\n",
    "        lr_str = \" | \".join([f\"{lr:.6e}\" for lr in lrs])\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} || \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | F1_micro: {avg_train_f1_micro:.4f} | \"\n",
    "              f\"F1_macro: {avg_train_f1_macro:.4f} | BLEU: {avg_train_bleu:.4f} || \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | F1_micro: {avg_val_f1_micro:.4f} | \"\n",
    "              f\"F1_macro: {avg_val_f1_macro:.4f} | BLEU: {avg_val_bleu:.4f} || \"\n",
    "              f\"LRs: {lr_str}\")\n",
    "\n",
    "        # --- чекпоинт ---\n",
    "        if avg_val_bleu > best_val_bleu:\n",
    "            best_val_bleu = avg_val_bleu\n",
    "            save_path = os.path.join(save_dir, f\"epoch{epoch+1}_valBLEU{avg_val_bleu:.4f}.pt\")\n",
    "            torch.save({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"scheduler_state\": scheduler.state_dict(),\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"val_bleu\": avg_val_bleu,\n",
    "            }, save_path)\n",
    "            print(f\"💾 Сохранён лучший чекпоинт: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c653c81-9358-4126-9280-60577129bbcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T09:48:16.489482Z",
     "iopub.status.busy": "2025-10-01T09:48:16.489022Z",
     "iopub.status.idle": "2025-10-01T09:48:17.280597Z",
     "shell.execute_reply": "2025-10-01T09:48:17.261988Z",
     "shell.execute_reply.started": "2025-10-01T09:48:16.489454Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoints/epoch191_valBLEU0.3250.pt\")[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "083ea6ff-210d-4e13-951e-73f033cc89c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T09:56:20.465303Z",
     "iopub.status.busy": "2025-10-01T09:56:20.464778Z",
     "iopub.status.idle": "2025-10-01T09:56:20.492755Z",
     "shell.execute_reply": "2025-10-01T09:56:20.491965Z",
     "shell.execute_reply.started": "2025-10-01T09:56:20.465279Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "def debug_one_batch_monster(model, dataloader, tokenizer, device=\"cuda\", idx=0, thr=0.4):\n",
    "    model.eval()\n",
    "    batch = next(iter(dataloader))  # берём один батч\n",
    "\n",
    "    # --- На девайс ---\n",
    "    embeddings = batch[\"embeddings\"].to(device)          # [B, N, D]\n",
    "    labels = batch[\"labels\"].to(device)                  # [B, num_classes]\n",
    "    bart_labels = batch[\"bart_labels\"].to(device)        # [B, T]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            embeddings,\n",
    "            labels=labels,\n",
    "            bart_labels=bart_labels,\n",
    "            classification_loss=torch.nn.BCEWithLogitsLoss()\n",
    "        )\n",
    "\n",
    "    # === КЛАССИФИКАЦИЯ ===\n",
    "    true_labels = labels[idx].cpu().numpy()\n",
    "    pred_logits = outputs[\"logits_cls\"][idx].cpu()\n",
    "    pred_probs = torch.sigmoid(pred_logits).numpy()\n",
    "    pred_classes = (pred_probs > thr).astype(int)\n",
    "\n",
    "    # === ГЕНЕРАЦИЯ ТЕКСТА ===\n",
    "    # правильный отчёт (референс)\n",
    "    refs = bart_labels[idx].detach().cpu().tolist()\n",
    "    refs = [tok for tok in refs if tok is not None and tok != -100]\n",
    "    ref_text = tokenizer.decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # генерация (beam search)\n",
    "    bridge_out = outputs.get(\"bart_out\", None)\n",
    "    if bridge_out is None:\n",
    "        # если в out не сохранили, достанем заново\n",
    "        _, gen_repr = model.bridge(embeddings)\n",
    "        enc_out = BaseModelOutput(last_hidden_state=gen_repr)\n",
    "    else:\n",
    "        enc_out = BaseModelOutput(last_hidden_state=outputs[\"bart_out\"].encoder_last_hidden_state)\n",
    "\n",
    "    generated_ids = model.bart.generate(\n",
    "        encoder_outputs=enc_out,\n",
    "        max_length=1024,\n",
    "        num_beams=4\n",
    "    )\n",
    "    hyp_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # === ВЫВОД ===\n",
    "    print(\"\\n=== Классификация ===\")\n",
    "    print(\"Истинные классы:\", true_labels)\n",
    "    print(\"Предсказанные :\", pred_classes)\n",
    "    print(\"Вероятности   :\", pred_probs.round(3))\n",
    "\n",
    "    print(\"\\n=== Тексты ===\")\n",
    "    print(\"GT :\", ref_text)\n",
    "    print(\"PR :\", hyp_text)\n",
    "\n",
    "    return {\n",
    "        \"true_labels\": true_labels,\n",
    "        \"pred_classes\": pred_classes,\n",
    "        \"pred_probs\": pred_probs,\n",
    "        \"ref_text\": ref_text,\n",
    "        \"hyp_text\": hyp_text\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9336f22-6b17-4793-8413-e1041a73edb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T09:57:13.688010Z",
     "iopub.status.busy": "2025-10-01T09:57:13.687528Z",
     "iopub.status.idle": "2025-10-01T09:58:26.304038Z",
     "shell.execute_reply": "2025-10-01T09:58:26.303033Z",
     "shell.execute_reply.started": "2025-10-01T09:57:13.687988Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Классификация ===\n",
      "Истинные классы: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Предсказанные : [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Вероятности   : [0.003 0.002 0.    0.008 0.001 0.    0.001 0.065 0.001 0.1   0.04  0.53\n",
      " 0.    0.001 0.    0.004 0.    0.028]\n",
      "\n",
      "=== Тексты ===\n",
      "GT : Trachea, both main bronchi are open. Mediastinal main vascular structures, heart contour, size are normal. Thoracic aorta diameter is normal. Pericardial effusion-thickening was not observed. Thoracic esophageal calibration was normal and no significant tumoral wall thickening was detected. No enlarged lymph nodes in prevascular, pre-paratracheal, subcarinal or bilateral hilar-axillary pathological dimensions were detected. When examined in the lung parenchyma window; A few millimetric nonspecific nodules and mild recessions are observed in the upper lobe and lower lobe of the right lung. Aeration of both lung parenchyma is normal and no infiltrative lesion is detected in the lung parenchyma. Pleural effusion-thickening was not detected. Upper abdominal organs included in the sections are normal. No space-occupying lesion was detected in the liver that entered the cross-sectional area. Bilateral adrenal glands were normal and no space-occupying lesion was detected. Bone structures in the study area are natural. Vertebral corpus heights are preserved.  A few millimetric nonspecific nodules and slight recessions in the upper lobe and lower lobe of the right lung.\n",
      "PR : Trachea, both main bronchi are open. Mediastinal main vascular structures, heart contour, size are normal. Pericardial effusion-thickening was not observed. Thoracic esophagus calibration was normal and no significant tumoral wall thickening was detected. No enlarged lymph nodes in prevascular, pre-paratracheal, subcarinal or bilateral hilar-axillary pathological dimensions were detected. When examined in the lung parenchyma window; There are millimetric nonspecific nodules in both lungs. In the upper lobes of both lungs, there is a nodule with a diameter of 3 mm in the middle lobe of the right lung. There is no mass with distinguishable borders in the left lung upper lobe inferior lingular segment. There are no lytic-destructive lesions in the bone structures within the sections. No pathological wall thickness increase was detected in the sections within the borders. No lytic or destructive lesions were detected in bone structures. No space-occupying lesion was observed in the liver that entered the cross-sectional area. No lymph node was detected within the limits of the sections in pathological size and appearance. There was no mass lesion in the abdomen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'true_labels': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float32),\n",
       " 'pred_classes': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
       " 'pred_probs': array([3.0590175e-03, 2.1879892e-03, 1.3973824e-04, 7.6949741e-03,\n",
       "        5.9485430e-04, 1.1335634e-04, 5.6751986e-04, 6.5188758e-02,\n",
       "        9.0039906e-04, 1.0023847e-01, 3.9702304e-02, 5.2968460e-01,\n",
       "        4.8123969e-04, 1.0277594e-03, 3.2878856e-04, 3.5577000e-03,\n",
       "        3.3481632e-04, 2.8099500e-02], dtype=float32),\n",
       " 'ref_text': 'Trachea, both main bronchi are open. Mediastinal main vascular structures, heart contour, size are normal. Thoracic aorta diameter is normal. Pericardial effusion-thickening was not observed. Thoracic esophageal calibration was normal and no significant tumoral wall thickening was detected. No enlarged lymph nodes in prevascular, pre-paratracheal, subcarinal or bilateral hilar-axillary pathological dimensions were detected. When examined in the lung parenchyma window; A few millimetric nonspecific nodules and mild recessions are observed in the upper lobe and lower lobe of the right lung. Aeration of both lung parenchyma is normal and no infiltrative lesion is detected in the lung parenchyma. Pleural effusion-thickening was not detected. Upper abdominal organs included in the sections are normal. No space-occupying lesion was detected in the liver that entered the cross-sectional area. Bilateral adrenal glands were normal and no space-occupying lesion was detected. Bone structures in the study area are natural. Vertebral corpus heights are preserved.  A few millimetric nonspecific nodules and slight recessions in the upper lobe and lower lobe of the right lung.',\n",
       " 'hyp_text': 'Trachea, both main bronchi are open. Mediastinal main vascular structures, heart contour, size are normal. Pericardial effusion-thickening was not observed. Thoracic esophagus calibration was normal and no significant tumoral wall thickening was detected. No enlarged lymph nodes in prevascular, pre-paratracheal, subcarinal or bilateral hilar-axillary pathological dimensions were detected. When examined in the lung parenchyma window; There are millimetric nonspecific nodules in both lungs. In the upper lobes of both lungs, there is a nodule with a diameter of 3 mm in the middle lobe of the right lung. There is no mass with distinguishable borders in the left lung upper lobe inferior lingular segment. There are no lytic-destructive lesions in the bone structures within the sections. No pathological wall thickness increase was detected in the sections within the borders. No lytic or destructive lesions were detected in bone structures. No space-occupying lesion was observed in the liver that entered the cross-sectional area. No lymph node was detected within the limits of the sections in pathological size and appearance. There was no mass lesion in the abdomen'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_one_batch_monster(model, val_loader, tokenizer, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d873d-2d18-4d67-b8c2-b0189a24d061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
